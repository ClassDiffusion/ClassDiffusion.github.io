<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
    <!-- Replace the content tag with appropriate information -->
    <meta name="description" content="DESCRIPTION META TAG" />
    <meta property="og:title" content="SOCIAL MEDIA TITLE TAG" />
    <meta
      property="og:description"
      content="SOCIAL MEDIA DESCRIPTION TAG TAG"
    />
    <meta property="og:url" content="URL OF THE WEBSITE" />
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
    <meta property="og:image" content="static/image/your_banner_image.png" />
    <meta property="og:image:width" content="1200" />
    <meta property="og:image:height" content="630" />

    <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG" />
    <meta
      name="twitter:description"
      content="TWITTER BANNER DESCRIPTION META TAG"
    />
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
    <meta
      name="twitter:image"
      content="static/images/your_twitter_banner_image.png"
    />
    <meta name="twitter:card" content="summary_large_image" />
    <!-- Keywords for your paper to be indexed by-->
    <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>
      ClassDiffusion
    </title>
    <link rel="icon" type="image/x-icon" href="static/images/favicon.ico" />
    <link
      href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
      rel="stylesheet"
    />

    <link rel="stylesheet" href="static/css/bulma.min.css" />
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css" />
    <link rel="stylesheet" href="static/css/bulma-slider.min.css" />
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css" />
    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css"
    />
    <link rel="stylesheet" href="static/css/index.css" />

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
    <script defer src="static/js/fontawesome.all.min.js"></script>
    <script src="static/js/bulma-carousel.min.js"></script>
    <script src="static/js/bulma-slider.min.js"></script>
    <script src="static/js/index.js"></script>
  </head>
  <body>
    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title">
                ClassDiffusion: More Aligned Personalization Tuning with Explicit Class Guidance
              </h1>
              <div class="is-size-5 publication-authors">
                <!-- Paper authors -->
                <span class="author-block">
                  <a href="https://rbrq03.github.io/" target="_blank"
                    >Jiannan Huang</a
                  ><sup>1, 3</sup>,</span
                >
                <span class="author-block">
                  <a href="https://scholar.google.com.sg/citations?user=8gm-CYYAAAAJ" target="_blank"
                    >Jun Hao Liew</a
                  ><sup>2</sup>,</span
                >
                <span class="author-block">
                  <a href="https://hanshuyan.github.io/" target="_blank"
                    >Hanshu Yan</a
                  ><sup>2</sup>,</span
                >
                <span class="author-block">
                  <a href="https://yuyangyin.github.io/" target="_blank"
                    >Yuyang Yin</a
                  ><sup>1</sup>,</span
                >
                <span class="author-block">
                  <a href="http://mepro.bjtu.edu.cn/zhaoyao/index.htm" target="_blank"
                    >Yao Zhao</a
                  ><sup>1</sup>,</span
                >
                <span class="author-block">
                  <a href="https://www.humphreyshi.com/" target="_blank"
                    >Humphrey Shi</a
                  ><sup>3</sup>,</span
                >
                <span class="author-block">
                  <a href="https://weiyc.github.io/index.html" target="_blank"
                    >Yunchao Wei</a
                  ><sup>1</sup>,</span
                >
                </span>
              </div>

              <div class="is-size-5 publication-authors">
                <span class="author-block"
                  ><sup>1</sup>Beijing Jiaotong University<br /><sup>2</sup>ByteDance Inc.<br /><sup>3</sup>SHI Labs@Georgia Tech</span
                >
              </div>

              <div style="color: red; font-style: italic; font-size: 1.3em; margin-top: 20px;">
                ðŸŽ‰ Accepted By ICLR25
              </div>              

              <div class="column has-text-centered">
                <div class="publication-links">
                  <!-- Arxiv PDF link -->
                  <span class="link-block">
                    <a
                      href="https://arxiv.org/pdf/2405.17532"
                      target="_blank"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>

                  <!-- ArXiv abstract Link -->
                  <span class="link-block">
                    <a
                      href="https://arxiv.org/abs/2405.17532"
                      target="_blank"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <span>arXiv</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a
                      href="https://github.com/Rbrq03/ClassDiffusion"
                      target="_blank"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- Teaser image-->
    <section class="hero teaser">
      <div class="container is-max-desktop">
        <div class="hero-body">
          <img src="static/images/story_page-0001.jpg" alt="teaser image" />
          <h2 class="subtitle has-text-centered">
            A qualitative result of two small stories produced by our model. The above showcases a bear's literary journey: from reading a book to ultimately earning a Nobel Literature Prize. The below shows the fate of a sunglasses. Finally, the bear gets the sunglasses. It shows a potential real-world application due to our model's high performance.
          </h2>
        </div>
      </div>
    </section>
    <!-- End teaser video -->

    <!-- Paper abstract -->
    <section class="section hero is-light">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
              <p>
                Recent text-to-image customization works have proven successful in generating images of given concepts by fine-tuning the diffusion models on a few examples. However, tuning-based methods inherently tend to overfit the concepts, resulting in failure to create the concept under multiple conditions (<i>e.g.</i>, headphone is missing when generating "a <tt>&lt;sks&gt;</tt> dog wearing a headphone"). Interestingly, we notice that the base model before fine-tuning exhibits the capability to compose the base concept with other elements (<i>e.g.</i>, "a dog wearing a headphone"), implying that the compositional ability only disappears after personalization tuning. We observe a semantic shift in the customized concept after fine-tuning, indicating that the personalized concept is not aligned with the original concept, and further show through theoretical analyses that this semantic shift leads to increased difficulty in sampling the joint conditional probability distribution, resulting in the loss of the compositional ability. Inspired by this finding, we present <b>ClassDiffusion</b>, a technique that leverages a <b>semantic preservation loss</b> to explicitly regulate the concept space when learning the new concept. Although simple, this approach effectively prevents semantic drift during the fine-tuning process on the target concepts. Extensive qualitative and quantitative experiments demonstrate that the use of semantic preservation loss effectively improves the compositional abilities of fine-tuning models. Lastly, we also extend our ClassDiffusion to personalized video generation, demonstrating its flexibility.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- End paper abstract -->

    <!-- <section class="hero teaser">
      <div class="container is-max-desktop">
        <div class="hero-body">
          <img src="static/images/method_new_page-0001.jpg" alt="teaser image" />
          <h2 class="subtitle has-text-centered">
            Overview of our proposed ClassDiffusion. Our semantic preservation loss (SPL) is calculated by measuring the cosine distance between text features extracted from the same text transformer (using EOS tokens as text features following CLIP) for phrases with personalized tokens and phrases with only superclasses.
          </h2>
        </div>
      </div>
    </section> -->

    <!-- Image carousel -->
    <!-- <section class="hero is-small">
      <div class="hero-body">
        <div class="container"> 
          <h2 class="title is-3">Single Concept Personalization</h2>
          <div id="results-carousel" class="carousel results-carousel">
            <div class="item">
              <img src="static/images/carousel1.jpg" alt="MY ALT TEXT" />
              <h2 class="subtitle has-text-centered">
                First image description.
              </h2>
            </div>
            <div class="item">
              <img src="static/images/method_new_page-0001.jpg" alt="MY ALT TEXT" />
              <h2 class="subtitle has-text-centered">
                Overview of our proposed ClassDiffusion. Our semantic preservation loss (SPL) is calculated by measuring the cosine distance between text features extracted from the same text transformer (using EOS tokens as text features following CLIP) for phrases with personalized tokens and phrases with only superclasses.
              </h2>
            </div>
            <div class="item">
              <img src="static/images/carousel3.jpg" alt="MY ALT TEXT" />
              <h2 class="subtitle has-text-centered">
                Third image description.
              </h2>
            </div>
            <div class="item">
              <img src="static/images/carousel4.jpg" alt="MY ALT TEXT" />
              <h2 class="subtitle has-text-centered">
                Fourth image description.
              </h2>
            </div>
          </div>
        </div>
      </div>
    </section> -->
    <!-- End image carousel -->

    <section class="section is-small">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full">
            <div class="content">
              <h2 class="title is-3">Single Concept Comparison</h2>
              <center>
              <img src="static/images/singleconcept.png" alt="Inference Overview" class="center-image">
              </center>
              <div class="level-set has-text-justified">
                <p>
                  Qualitative comparison between our method and baselines with single given concept.
                </p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="section is-small">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full">
            <div class="content">
              <h2 class="title is-3">Multiple Concepts Comparison</h2>
              <center>
              <img src="static/images/multiple.png" alt="Inference Overview" class="center-image">
              </center>
              <div class="level-set has-text-justified">
                <p>
                  Qualitative comparison between our method and custom diffusion(CD) with multiple given concept.
                </p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>


    <section class="section is-small">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full">
            <div class="content">
              <h2 class="title is-3">Personalized Video</h2>
              <div id="results-carousel" class="carousel results-carousel">
                <div class="item">
                  <div class="media-container">
                    <img src="static/images/dog.jpeg" alt="MY ALT TEXT" />
                    <video autoplay loop muted>
                      <source src="static/images/dog.mp4" type="video/mp4">
                    </video>
                  </div>
                  <p class="subtitle has-text-centered">
                    A dog running on the street
                  </p>
                </div>
                <div class="item">
                  <div class="media-container">
                    <img src="static/images/duck_toy.jpg" alt="MY ALT TEXT" />
                    <video autoplay loop muted>
                      <source src="static/images/duck_toy.mp4" type="video/mp4">
                    </video>
                  </div>
                  <p class="subtitle has-text-centered">
                    A duck toy floating on the water
                  </p>
                </div>
                <div class="item">
                  <div class="media-container">
                    <img src="static/images/cat.jpg" alt="MY ALT TEXT" />
                    <video autoplay loop muted>
                      <source src="static/images/cat.mp4" type="video/mp4">
                    </video>
                  </div>
                  <p class="subtitle has-text-centered">
                    A cat chasing a ball
                  </p>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>
    
    <style>
      .media-container {
        display: flex;
        justify-content: space-between;
        align-items: center;
        margin-bottom: 1rem;
      }
    
      .media-container img, .media-container video {
        width: 48%;
        height: auto;
      }
    
      .subtitle.has-text-centered {
        margin-top: 0.5rem;
        font-family: 'Comic Sans MS', 'Comic Sans', cursive, sans-serif;
      }
    </style>
    

    <!-- <section class="section is-small">
      <div class="hero-body">
        <div class="container"> 
          <h2 class="title is-3">Analysis</h2>
          <div id="results-carousel" class="carousel results-carousel">
            <div class="item">
              <img src="static/images/exp_ob.png" alt="MY ALT TEXT" />
              <h2 class="subtitle has-text-centered">
                (a) Each dot represents the CLIP text embedding of a phrase combining an adjective and &quot; dog &quot; (<i>e.g.</i>, a cute dog). After fine-tuning, customized concepts (the blue dot represents the concept before fine-tuning, and the red dot represents the one after) become far away from the center of the &quot;dog&quot; distribution in the text feature space. (b) Visualization results of cross-attention maps corresponding to the dog token when using the prompt &quot;a photo of a dog swimming in the pool &quot;.
              </h2>
            </div>
            <div class="item">
              <img src="static/images/theoretical_page-0001.jpg" alt="MY ALT TEXT" />
              <h2 class="subtitle has-text-centered">
                Overview of our proposed ClassDiffusion. Our semantic preservation loss (SPL) is calculated by measuring the cosine distance between text features extracted from the same text transformer (using EOS tokens as text features following CLIP) for phrases with personalized tokens and phrases with only superclasses.
              </h2>
            </div>
          </div>
        </div>
      </div>
    </section> -->

    <section class="section is-small">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full">
            <div class="content">
              <h2 class="title is-3">Emperical Analysis</h2>
              <center>
              <img src="static/images/exp_ob.png" alt="Inference Overview" class="center-image">
              </center>
              <div class="level-set has-text-justified">
                <p>
                  (a) Each dot represents the position of a phrase combining an adjective and "dog" in the CLIP text-space. After fine-tuning, customized concepts move further away from the the distribution of super-class.(b) Visualization results of cross-attention map activation maps corresponding to the dog token. The bar chart on the right shows the average activation level in the dog area. Experiments show that the activation strengths of the corresponding classes decrease with the increase of the learning rate and the total number of training steps.
                  These demonstrate that the customized concepts likely no longer belong to the super-class, resulting in a loss of super-class semantic information, such as wearing a headphone. 
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="section is-small">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full">
            <div class="content">
              <h2 class="title is-3">Theoretical Analysis</h2>
              <center>
              <img src="static/images/theoretical_page-0001.jpg" alt="Inference Overview" class="center-image">
              </center>
              <div class="level-set has-text-justified">
                <p>
                  The orange and green point sets represent the distributions of dogs and headphones, respectively, and their overlapping regions represent their joint probability distributions. During the tuning process, the conditional distribution of dogs and headphones shrinks, which gradually increases the difficulty of sampling. Unlike the Prior Preservation Loss (PPL) in DreamBooth which aims to maintain class diversity, our proposed Semantic Preservation Loss (SPL) focuses on recovering the semantic space of the customized concept. This approach enables our method to synthesize images that are more consistent with the text prompt.
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="section is-small">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full">
            <div class="content">
              <h2 class="title is-3">Method Overview</h2>
              <center>
              <img src="static/images/method_new_page-0001.jpg" alt="Inference Overview" class="center-image">
              </center>
              <div class="level-set has-text-justified">
                <p>
                  The framework of ClassDiffusion. The personalization fine-tuning strategy is based on Custom Diffusion, which primarily fine-tunes the K and V parameters in the transformer block. Our <b>semantic preservation loss (SPL)</b> is calculated by measuring the cosine distance between text features extracted from the same text transformer (using EOS tokens as text features following CLIP) for phrases with personalized tokens and phrases with only super-class.
                </p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- Youtube video -->
    <!-- <section class="hero is-small is-light">
      <div class="hero-body">
        <div class="container"> -->
          <!-- Paper video. -->
          <!-- <h2 class="title is-3">Video Presentation</h2>
          <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
              <div class="publication-video"> -->
                <!-- Youtube embed code here -->
                <!-- <iframe
                  src="https://www.youtube.com/embed/JkaxUblCGz0"
                  frameborder="0"
                  allow="autoplay; encrypted-media"
                  allowfullscreen
                ></iframe>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section> -->
    <!-- End youtube video -->

    <!-- Video carousel -->
    <!-- <section class="hero is-small">
      <div class="hero-body">
        <div class="container">
          <h2 class="title is-3">Another Carousel</h2>
          <div id="results-carousel" class="carousel results-carousel">
            <div class="item item-video1">
              <video
                poster=""
                id="video1"
                autoplay
                controls
                muted
                loop
                height="100%"
              > -->
                <!-- Your video file here -->
                <!-- <source src="static/videos/carousel1.mp4" type="video/mp4" />
              </video>
            </div>
            <div class="item item-video2">
              <video
                poster=""
                id="video2"
                autoplay
                controls
                muted
                loop
                height="100%"
              > -->
                <!-- Your video file here -->
                <!-- <source src="static/videos/carousel2.mp4" type="video/mp4" />
              </video>
            </div>
            <div class="item item-video3">
              <video
                poster=""
                id="video3"
                autoplay
                controls
                muted
                loop
                height="100%"
              >
                \ -->
                <!-- Your video file here -->
                <!-- <source src="static/videos/carousel3.mp4" type="video/mp4" />
              </video>
            </div>
          </div>
        </div>
      </div>
    </section> -->
    <!-- End video carousel -->

    <!-- Paper poster -->
    <!-- <section class="hero is-small is-light">
      <div class="hero-body">
        <div class="container">
          <h2 class="title">Poster</h2>

          <iframe src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        </div>
      </div>
    </section> -->
    <!--End paper poster -->

    <!--BibTex citation -->
    <section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <pre><code>@article{huang2024classdiffusion,
  title={ClassDiffusion: More Aligned Personalization Tuning with Explicit Class Guidance},
  author={Huang, Jiannan and Liew, Jun Hao and Yan, Hanshu and Yin, Yuyang and Zhao, Yao and Wei, Yunchao},
  journal={arXiv preprint arXiv:2405.17532},
  year={2024}
}</code></pre>
      </div>
    </section>
    <!--End BibTex citation -->

    <footer class="footer">
      <div class="container">
        <div class="columns is-centered">
          <div class="column is-8">
            <div class="content">
              <p>
                This page was built using the
                <a
                  href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                  target="_blank"
                  >Academic Project Page Template</a
                >
                which was adopted from theÂ <a
                  href="https://nerfies.github.io"
                  target="_blank"
                  >Nerfies</a
                >Â project page. You are free to borrow the of this website, we
                just ask that you link back to this page in the footer. <br />
                This website is licensed under a
                <a
                  rel="license"
                  href="http://creativecommons.org/licenses/by-sa/4.0/"
                  target="_blank"
                  >Creative Commons Attribution-ShareAlike 4.0 International
                  License</a
                >.
              </p>
            </div>
          </div>
        </div>
      </div>
    </footer>

    <!-- Statcounter tracking code -->

    <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->
  </body>
</html>
